# ğŸ”¬ Rijeesh Keloth - Research Assistant (RAG)

A **Retrieval-Augmented Generation** application that allows anyone to ask questions about my research publications and get accurate, context-aware answers.

## ğŸ¯ What This Demonstrates

This project showcases:
- **LLM API Integration** (Claude/OpenAI) in production
- **RAG Architecture** - retrieval + generation pipeline
- **Prompt Engineering** for accurate responses
- **Web Application Development** with Streamlit

## ğŸš€ Quick Start

### 1. Clone and Install

```bash
git clone https://github.com/rijeeshkeloth/research-rag.git
cd research-rag
pip install -r requirements.txt
```

### 2. Get an API Key

Choose one:
- **Anthropic Claude**: https://console.anthropic.com/
- **OpenAI GPT**: https://platform.openai.com/

### 3. Run Locally

```bash
streamlit run app.py
```

Open http://localhost:8501 in your browser.

## â˜ï¸ Deploy to Streamlit Cloud (Free)

1. Push this repo to GitHub
2. Go to https://streamlit.io/cloud
3. Click "New app" â†’ Select your repo
4. Deploy!

Users will enter their own API keys in the sidebar.

## ğŸ“ Project Structure

```
research-rag/
â”œâ”€â”€ app.py              # Main Streamlit application
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ README.md          # This file
â””â”€â”€ publications/      # (Optional) PDF storage
    â”œâ”€â”€ solid_2024.pdf
    â””â”€â”€ ...
```

## ğŸ”§ How It Works

```
User Question
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. RETRIEVAL   â”‚  Search publications for relevant context
â”‚  (Keyword/Embed)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. AUGMENT     â”‚  Build prompt with retrieved context
â”‚  (Context Build)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. GENERATE    â”‚  LLM generates answer from context
â”‚  (Claude/GPT)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Answer + Sources
```

## ğŸ“š Publications Indexed

| Paper | Year | Collaboration | My Role |
|-------|------|---------------|---------|
| Search for Very-Short-Baseline Oscillations... | 2024 | SoLid | Analysis Lead |
| DarkSide-20k sensitivity to light dark matter | 2024 | DarkSide-20k | Software Lead |
| Improved Measurement of Neutrino Oscillations | 2022 | NOvA | ML Developer |
| Search for Heavy Neutral Leptons | 2024 | SoLid | ML Classifier |
| SoLid Detector Calibration | 2021 | SoLid | Calibration Lead |

## ğŸ› ï¸ Customization

### Add More Publications

Edit the `PUBLICATIONS` list in `app.py`:

```python
PUBLICATIONS.append({
    "id": "new_paper",
    "title": "Your Paper Title",
    "arxiv": "2401.xxxxx",
    "year": 2024,
    "journal": "Journal Name",
    "collaboration": "Collaboration",
    "abstract": "Paper abstract...",
    "your_contribution": "What you did...",
    "keywords": ["keyword1", "keyword2"]
})
```

### Use Semantic Search (Recommended)

For better search quality, uncomment the embedding code and install:

```bash
pip install sentence-transformers chromadb
```

### Add PDF Processing

To ingest actual PDFs:

```bash
pip install pypdf langchain
```

Then use LangChain's PDF loader to chunk and embed documents.

## ğŸ¨ Screenshots

*Add screenshots of your deployed app here*

## ğŸ“ Example Questions

- "What is your contribution to the SoLid experiment?"
- "Tell me about the machine learning work you've done"
- "What is DarkSide-20k?"
- "Explain the sterile neutrino search"
- "What systematic uncertainties did you handle?"

## ğŸ”’ Security Notes

- API keys are entered by users, not stored
- No sensitive data in the codebase
- Streamlit Cloud handles secrets securely

## ğŸ“« Contact

- **Email**: rijeesh@vt.edu
- **LinkedIn**: [linkedin.com/in/rijeeshkeloth](https://linkedin.com/in/rijeeshkeloth)
- **INSPIRE-HEP**: [inspirehep.net/authors/1454963](https://inspirehep.net/authors/1454963)

---

*Built as a portfolio project demonstrating LLM integration and RAG architecture.*
